{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "heartPredict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bentonluu/CPSC501-Assignment4-TensorFlow/blob/master/heartPredict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce8DvOGmjghN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMhJ2Njsjgyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload heart.csv dataset file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvZ42jOyjkXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import of all libraries needed \n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from keras import regularizers\n",
        "from numpy.random import RandomState\n",
        "import functools\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk9xCKKVSc3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note that parts of code were referenced from: \n",
        "# https://www.tensorflow.org/tutorials/load_data/csv \n",
        "# https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\n",
        "\n",
        "LABEL_COLUMN = 'chd'\n",
        "TRAIN_CSV_PATH = 'heart_train.csv'\n",
        "TEST_CSV_PATH = 'heart_test.csv'\n",
        "\n",
        "heart_df = pd.read_csv(\"heart.csv\")\n",
        "\n",
        "# Splits the 'heart.csv' data into train and test data. 80% of the samples are \n",
        "# used for training and the other 20% used for testing.\n",
        "train_data = heart_df.sample(frac=0.80, random_state=RandomState())\n",
        "test_data = heart_df.loc[~heart_df.index.isin(train_data.index)]\n",
        "\n",
        "train_data.to_csv(TRAIN_CSV_PATH, index=False)\n",
        "test_data.to_csv(TEST_CSV_PATH, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53eTWmWtNMMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creates a dataset from the CSV file\n",
        "def get_dataset(file_path, **kwargs):\n",
        "  dataset = tf.data.experimental.make_csv_dataset(file_path, batch_size=5, label_name=LABEL_COLUMN)\n",
        "  return dataset\n",
        "\n",
        "# Used to center the data within the feature's range\n",
        "def normalize_numeric_data(data, mean, std):\n",
        "  return (data-mean)/std\n",
        "\n",
        "# Packs the feature columns into a vector for the model\n",
        "class PackNumericFeatures(object):\n",
        "  def __init__(self, names):\n",
        "    self.names = names\n",
        "\n",
        "  def __call__(self, features, labels):\n",
        "    numeric_features = [features.pop(name) for name in self.names]\n",
        "    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
        "    numeric_features = tf.stack(numeric_features, axis=-1)\n",
        "    features['numeric'] = numeric_features\n",
        "\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWVKpZ7PWsbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CSV_FEATURES = ['sbp', 'tobacco', 'ldl', 'adiposity', 'typea','obesity','alcohol','age']\n",
        "\n",
        "# CSV file data is converted into dataset to train model\n",
        "raw_train_data = get_dataset(TRAIN_CSV_PATH)\n",
        "raw_test_data = get_dataset(TEST_CSV_PATH)\n",
        "\n",
        "# Converts the dataset into a vector for the model\n",
        "packed_train_data = raw_train_data.map(PackNumericFeatures(CSV_FEATURES))\n",
        "packed_test_data = raw_test_data.map(PackNumericFeatures(CSV_FEATURES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCd7O5wpj2Pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "desc = pd.read_csv(TRAIN_CSV_PATH)[CSV_FEATURES].describe()\n",
        "\n",
        "# Finds the global mean of each of the features in the dataset\n",
        "MEAN = np.array(desc.T['mean'])\n",
        "# Finds the global standard deviation of each of the features in the data\n",
        "STD = np.array(desc.T['std'])\n",
        "\n",
        "# Normalize the continuous data\n",
        "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
        "numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(CSV_FEATURES)])\n",
        "numeric_columns = [numeric_column]\n",
        "\n",
        "# Convert the categorical label data into values that can be used by the model\n",
        "CATEGORIES = { 'famhist': ['Present', 'Absent'] }\n",
        "categorical_columns = []\n",
        "for feature, vocab in CATEGORIES.items():\n",
        "  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "        key=feature, vocabulary_list=vocab)\n",
        "  categorical_columns.append(tf.feature_column.indicator_column(cat_col))\n",
        "\n",
        "# Creates the input layer of both the continuous and categorical data\n",
        "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns + numeric_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NWoCaAlawU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the model, training the model and evaluating the model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  preprocessing_layer,\n",
        "  tf.keras.layers.Dense(128, kernel_regularizer=regularizers.l2(0.0001), activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(128, kernel_regularizer=regularizers.l2(0.0001), activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"--Fit model--\")\n",
        "model.fit(packed_train_data, epochs=10, verbose=2, steps_per_epoch=64)\n",
        "\n",
        "print(\"--Evaluate model--\")\n",
        "model_loss, model_acc = model.evaluate(packed_test_data, verbose=2, steps=64)\n",
        "print(f\"Model Loss:    {model_loss:.2f}\")\n",
        "print(f\"Model Accuray: {model_acc*100:.1f}%\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}